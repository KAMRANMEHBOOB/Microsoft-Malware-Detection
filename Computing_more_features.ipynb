{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CbCfIj1F1BL"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Process# this is used for multithreading\n",
    "import multiprocessing\n",
    "import codecs# this is used for file operations \n",
    "import random as r\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tZRSiDYNQLSr",
    "outputId": "198a08cc-450b-43a3-edcd-4b5b8ee63d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Computing bigrams</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "87T5BetZQYQv",
    "outputId": "9d23b253-ca95-4022-adb4-06dde2cf660a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [00:00, 422.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "byte_vocab = \"00,01,02,03,04,05,06,07,08,09,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??\"\n",
    "from tqdm import tqdm\n",
    "def byte_bigram():    \n",
    "    byte_bigram_vocab = []\n",
    "    for i, v in tqdm(enumerate(byte_vocab.split(','))):\n",
    "        #print(i,v,len(byte_vocab.split(',')))\n",
    "        for j in range(0, len(byte_vocab.split(','))):\n",
    "            byte_bigram_vocab.append(v + ' ' +byte_vocab.split(',')[j])\n",
    "    print(len(byte_bigram_vocab))\n",
    "    return byte_bigram_vocab\n",
    "\n",
    "bigram=byte_bigram()\n",
    "\n",
    "def converttostr(input_seq, seperator):\n",
    "    \n",
    "   # Join all the strings in list\n",
    "    final_str = input_seq.replace(\" \", \"\")\n",
    "    return final_str\n",
    "\n",
    "def space2comma(input_seq):\n",
    "    \n",
    "   # Join all the strings in list\n",
    "    final_str = input_seq.replace(\" \",\",\")\n",
    "    return final_str\n",
    "\n",
    "seperator = ''\n",
    "for i in range(len(bigram)):\n",
    "    bigram[i] = converttostr(bigram[i], seperator)\n",
    "    \n",
    "result= ''\n",
    "for i in bigram:\n",
    "    result += str(i)+\" \"\n",
    "\n",
    "result = 'ID '+result\n",
    "result = space2comma(result)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "byte_feature_file=open('bigramss.csv','w+')\n",
    "k=0\n",
    "byte_feature_file.write(result)\n",
    "byte_feature_file.write(\"\\n\")\n",
    "\n",
    "for file in tqdm(files):\n",
    "    \n",
    "    #print(file)\n",
    "    filenames2.append(file)\n",
    "    byte_feature_file.write(file+\",\")\n",
    "    if(file.endswith(\"txt\")):\n",
    "        \n",
    "        with open('byteFiles/'+file,\"r\") as byte_flie:\n",
    "            \n",
    "            for lines in byte_flie:                \n",
    "                line=lines.rstrip().split(\" \")                \n",
    "                for hex_code in range(len(line)-1):\n",
    "                    #print(line[hex_code],line[hex_code+1])\n",
    "                    str1 = line[hex_code] + line[hex_code+1]\n",
    "                    \n",
    "                    if( (line[hex_code] == \"??\") and (line[hex_code] != line[hex_code +1])):\n",
    "                        index2 = int(line[hex_code + 1],16)\n",
    "                        feature_matrix[k][65792+index2]+=1\n",
    "                        \n",
    "                    elif((line[hex_code+1] == \"??\") and (line[hex_code+1] != line[hex_code])):\n",
    "                        index2 = int(line[hex_code],16)\n",
    "                        key = line[hex_code] + 'ff'\n",
    "                        key = int(key,16)\n",
    "                        feature_matrix[k][key+1+index2]+=1\n",
    "                        \n",
    "                        feature_matrix[k][index2 + 256]+=1\n",
    "                    elif((line[hex_code+1] == \"??\") and (line[hex_code+1] == line[hex_code])):\n",
    "                        feature_matrix[k][66048]\n",
    "                        \n",
    "                    else:\n",
    "                        index2 = int(str1,16)\n",
    "                        key = int(line[hex_code],16)\n",
    "                        feature_matrix[k][key+ index2]+=1  \n",
    "                        \n",
    "                        \n",
    "        byte_flie.close()\n",
    "    for i, row in enumerate(feature_matrix[k]):\n",
    "        \n",
    "        if i!=len(feature_matrix[k])-1:            \n",
    "            row = int(row)  \n",
    "            #print(str(row))\n",
    "            byte_feature_file.write(str(row)+\",\")\n",
    "        else:\n",
    "            byte_feature_file.write(str(row))\n",
    "    byte_feature_file.write(\"\\n\")\n",
    "    \n",
    "    k += 1\n",
    "    \n",
    "byte_feature_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Steps Followed </h2>\n",
    "<p> I divided by bigramss.csv file into 6 smaller csv files in ubuntu and then converted them into sarse matrices and saved them as 1.npz etc.,</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import scipy.sparse\n",
    "matrix1 = scipy.sparse.load_npz(\"/content/drive/My Drive/Classroom/1.npz\")\n",
    "matrix2 = scipy.sparse.load_npz(\"/content/drive/My Drive/Classroom/2.npz\")\n",
    "matrix3 = scipy.sparse.load_npz(\"/content/drive/My Drive/Classroom/3.npz\")\n",
    "matrix4 = scipy.sparse.load_npz(\"/content/drive/My Drive/Classroom/4.npz\")\n",
    "matrix5 = scipy.sparse.load_npz(\"/content/drive/My Drive/Classroom/5.npz\")\n",
    "matrix6 = scipy.sparse.load_npz(\"/content/drive/My Drive/Classroom/6.npz\")\n",
    "\n",
    "from scipy.sparse import vstack\n",
    "outputmatrix = vstack((matrix1,matrix2,matrix3,matrix4,matrix5,matrix6))\n",
    "\n",
    "del matrix1,matrix2,matrix3,matrix4,matrix5,matrix6\n",
    "\n",
    "dataset=pd.DataFrame.sparse.from_spmatrix(outputmatrix,columns = bigram)\n",
    "\n",
    "\n",
    "col = ['ID']\n",
    "ID = []\n",
    "\n",
    "data = pd.read_csv('/content/drive/My Drive/Classroom/bigramss1.csv', usecols=col)\n",
    "ID.extend(data['ID'])\n",
    "del data\n",
    "data = pd.read_csv('/content/drive/My Drive/Classroom/bigramss2.csv', usecols=col)\n",
    "ID.extend(data['ID'])\n",
    "del data\n",
    "data = pd.read_csv('/content/drive/My Drive/Classroom/bigramss3.csv', usecols=col)\n",
    "ID.extend(data['ID'])\n",
    "del data\n",
    "data = pd.read_csv('/content/drive/My Drive/Classroom/bigramss4.csv', usecols=col)\n",
    "ID.extend(data['ID'])\n",
    "del data\n",
    "data = pd.read_csv('/content/drive/My Drive/Classroom/bigramss5.csv', usecols=col)\n",
    "ID.extend(data['ID'])\n",
    "del data\n",
    "data = pd.read_csv('/content/drive/My Drive/Classroom/bigramss6.csv', usecols=col)\n",
    "ID.extend(data['ID'])\n",
    "del data\n",
    "\n",
    "dataset.insert(loc=0, column='ID', value=ID)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "dataset = pd.read_csv('bigramss.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I deleted any column that contained all zeros</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weHyvKmPHI5l"
   },
   "outputs": [],
   "source": [
    "dataset.insert(loc=0, column='ID', value=ID)\n",
    "dataset = dataset.loc[:, (dataset != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Zk4nS-rTIaNO",
    "outputId": "fdb49c41-7d82-4a84-d31c-b7392e8985ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10868/10868 [00:00<00:00, 2220345.63it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset2 = pd.read_csv('/content/drive/My Drive/Classroom/trainLabels.csv')\n",
    "dataset2=dataset2.drop(['Id'],axis=1)\n",
    "classlabel = []\n",
    "for i in tqdm(dataset2['Class']):\n",
    "    classlabel.append(i)\n",
    "dataset.insert(loc=1, column='label', value=classlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('asmFiles')\n",
    "filenames2=[]\n",
    "#print(len(files))\n",
    "imagefeatures= np.zeros((len(files), 200))\n",
    "\n",
    "import array\n",
    "import scipy\n",
    "\n",
    "images = 'asm_image'\n",
    "\n",
    "os.makedirs(images)\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "def collect_img_asm():\n",
    "    for asmfile in tqdm(os.listdir(\"asmFiles\")):\n",
    "        #print(asmfile)\n",
    "        filename = asmfile.split('.')[0]\n",
    "        file = codecs.open(\"asmFiles/\"+ asmfile,'rb')\n",
    "        filelen = os.path.getsize(\"asmFiles/\"+ asmfile)\n",
    "        width =int(filelen ** 0.5)\n",
    "        rem =int(filelen / width)\n",
    "        arr = array.array('B')\n",
    "        arr.frombytes(file.read())\n",
    "        file.close()\n",
    "        reshaped = np.reshape(arr[:width * width], (width, width))\n",
    "        reshaped = np.uint8(reshaped)\n",
    "        scipy.misc.imsave('asm_image/'+ filename +'.png',reshaped)\n",
    "        \n",
    "\n",
    "collect_img_asm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "idd = []\n",
    "for i, asmfile in tqdm(enumerate(os.listdir(\"asmFiles\"))):\n",
    "    idd.append(asmfile.split('.')[0]+'.txt')    \n",
    "    img = cv2.imread(\"asm_image/\"+ asmfile.split('.')[0] +'.png')\n",
    "    img_arr = img.flatten()[:200]\n",
    "    imagefeatures[i, :] += img_arr\n",
    "    \n",
    "img_features_name= []\n",
    "for i in range(200):\n",
    "    img_features_name.append('pix'+str(i))\n",
    "    \n",
    "imgdf = pd.DataFrame(imagefeatures, columns = img_features_name)\n",
    "\n",
    "imgdf.to_csv('pixel.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFjoE1O7Xk7u"
   },
   "outputs": [],
   "source": [
    "#del dataset2,ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhRv9px4SDiR"
   },
   "outputs": [],
   "source": [
    "asmfile = pd.read_csv('/content/drive/My Drive/Classroom/pixel.csv')\n",
    "#asmfile.head(2)\n",
    "asmfile = asmfile.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqxLGnB0VWyU"
   },
   "outputs": [],
   "source": [
    "data2 = pd.merge(dataset,asmfile,on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96bosgoJGmCb"
   },
   "outputs": [],
   "source": [
    "del dataset, asmfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "B3c1lkvgeue4",
    "outputId": "e8c2a5d5-82b5-495c-88ce-935ac36ef405"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>size_asm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01azqd4InC7m9JpocGv5.txt</td>\n",
       "      <td>56.229886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB.txt</td>\n",
       "      <td>13.999378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID   size_asm\n",
       "0  01azqd4InC7m9JpocGv5.txt  56.229886\n",
       "1  01IsoiSMh5gxyDYTl4CB.txt  13.999378"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/content/drive/My Drive/Classroom/asm_with_size.csv\")\n",
    "dataset = dataset.drop(['Unnamed: 0','Class'],axis=1)\n",
    "dataset['ID'] = dataset['ID']+\".txt\"\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKmCDqXJgHHE"
   },
   "outputs": [],
   "source": [
    "data2 = pd.merge(data2,dataset,on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rFTVbISWmmfx",
    "outputId": "f9ebc662-9ba3-4eb7-9da5-6e29900fe658"
   },
   "outputs": [],
   "source": [
    "#data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "warG6Pgmft0Y"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/content/drive/My Drive/Classroom/result.csv\")\n",
    "dataset = dataset.drop(['Unnamed: 0','Class'],axis=1)\n",
    "dataset['ID'] = dataset['ID']+'.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPcgtemrhkhj"
   },
   "outputs": [],
   "source": [
    "data2 = pd.merge(dataset,data2,on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jgQDn6Pnn9Ku",
    "outputId": "27c5087c-3b03-48d4-ecf4-0083f858a111"
   },
   "outputs": [],
   "source": [
    "#len(data2['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgOUHSyYsgO_"
   },
   "outputs": [],
   "source": [
    "#del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> After merging the asmfile features with the byte file features , I took a sample of 6K to work with(due to memory constraints).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83xlRvUO77ri"
   },
   "outputs": [],
   "source": [
    "data2 = data2.sample(n = 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDwevg4eo3Tw"
   },
   "outputs": [],
   "source": [
    "data2 = data2.drop(['ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS6Nm0kXgnGM"
   },
   "outputs": [],
   "source": [
    "def normalize(df):    \n",
    "    result1 = df.copy()\n",
    "    for i in tqdm(df.columns):        \n",
    "        if (i != 'label'):\n",
    "            cc = df[i]\n",
    "            cc = list(cc)\n",
    "            max_value = np.max(cc)\n",
    "            min_value = np.min(cc)\n",
    "            result1[i] = (df[i] - min_value) / (max_value - min_value)\n",
    "return result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3FAicOHuump"
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "data2 = normalize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Js0Hdm3A3GEb"
   },
   "outputs": [],
   "source": [
    "data_y = data2['label']\n",
    "data2 = data2.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9Td_nev9KGfH",
    "outputId": "c31af425-fbb3-461e-d343-dbcce4fce39e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 15000)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I then selected 15k most useful features from the total number of features as suggested by the applied ai team</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQsx6N3pZ0Z9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "data2 = data2.fillna(0)\n",
    "data2 = SelectKBest(chi2, k=15000).fit_transform(data2, data_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIKWsmT4n4Mq"
   },
   "outputs": [],
   "source": [
    "#data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "iCTyf6kou6ZS"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data2, data_y,stratify=data_y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9NLjcxSvDWAW"
   },
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I used LightGBM instead of xgboost (xgboost was taking a lot of time to run) . Parameters for the classifier were provided by the team </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Y6GTcgwBqREf"
   },
   "outputs": [],
   "source": [
    "x_cfl = LGBMClassifier(learning_rate = 0.03,n_estimators = 3000,max_dept=11,min_child_weight=9,subsample=1,colsample_bytree=0.4,objective='binary:logistic',scale_pos_weight=1,reg_alpha=0.6,reg_lambda=3,seed=42,n_thread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "k2NHAfoAHtEz",
    "outputId": "32d96c58-362b-4170-dc13-98afe198650d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.4,\n",
       "               importance_type='split', learning_rate=0.03, max_dept=11,\n",
       "               max_depth=-1, min_child_samples=20, min_child_weight=9,\n",
       "               min_split_gain=0.0, n_estimators=3000, n_jobs=-1, n_thread=-1,\n",
       "               num_leaves=31, objective='binary:logistic', random_state=None,\n",
       "               reg_alpha=0.6, reg_lambda=3, scale_pos_weight=1, seed=42,\n",
       "               silent=True, subsample=1, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cfl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "X_X4wMkwaF2l"
   },
   "outputs": [],
   "source": [
    "sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "R0fCpt3yaIsj",
    "outputId": "ff3848bd-4d2f-4c2d-e644-2740751e94ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LGBMClassifier(boosting_type='gbdt',\n",
       "                                                     class_weight=None,\n",
       "                                                     colsample_bytree=0.4,\n",
       "                                                     importance_type='split',\n",
       "                                                     learning_rate=0.03,\n",
       "                                                     max_dept=11, max_depth=-1,\n",
       "                                                     min_child_samples=20,\n",
       "                                                     min_child_weight=9,\n",
       "                                                     min_split_gain=0.0,\n",
       "                                                     n_estimators=3000,\n",
       "                                                     n_jobs=-1, n_thread=-1,\n",
       "                                                     num_leaves=31,\n",
       "                                                     objective='binary:logistic',\n",
       "                                                     random_state=None,\n",
       "                                                     reg_alpha=0.6,\n",
       "                                                     reg_lambda=3,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=42, silent=True,\n",
       "                                                     subsample=1,\n",
       "                                                     subsample_for_bin=200000,\n",
       "                                                     subsample_freq=0),\n",
       "                       cv=None, method='sigmoid')"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BUtDTtwsaKPb"
   },
   "outputs": [],
   "source": [
    "predict_y = sig_clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "6gdea2Z_IEMW",
    "outputId": "43a88c50-9468-4baa-8e4c-aa8d4e8410e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is  0.011751647377874243\n",
      "Cross Validation loss is  0.012818226280167563\n",
      "test loss is  0.01369734213299497\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loss is \", log_loss(y_train,predict_y) )\n",
    "predict_y = sig_clf.predict_proba(X_cv)\n",
    "print(\"Cross Validation loss is \", log_loss(y_cv,predict_y))\n",
    "predict_y = sig_clf.predict_proba(X_test)\n",
    "print(\"test loss is \",log_loss(y_test,predict_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The logloss I got is 0.011 for train data, 0.012 for cv, 0.013 for test data. This shows that the model is not overfitting </p>\n",
    "<p> Due to memory constraints i was only able to process 6000 datapoints. The results will be better if we do it on all the datapoints</p>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Final_assignment16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
